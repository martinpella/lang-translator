{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinpella/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os, re, pickle, collections, bcolz, string\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to build a neural machine translation model to translate english to french. As this project is for study purpose, the scope is limited to english questions that start with 'Wh' (What, Who, Where, Why, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Corpus preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will use <a href='http://www.statmt.org/wmt10/training-giga-fren.tar'>French-English 109 corpus</a> dataset, crawled from Canadian and European Union sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "corpus_path = PATH + '/data/fr-en-109-corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fname = 'giga-fren.release2.fixed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "en_corpus_path = f'{corpus_path}/{fname}en' \n",
    "fr_corpus_path = f'{corpus_path}/{fname}fr' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's find all english sentences that start with 'Wh' and end with '?'. On the other hand, we want to find all french sentences that just end with '?'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
    "re_fq = re.compile('^([^?.!]+\\?)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lines = ((re_eq.search(eq), re_fq.search(fq)) for eq, fq in zip(open(en_corpus_path), open(fr_corpus_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "questions = [(en.group(), fr.group()) for en, fr in lines if en and fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52331"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have 52331 english questions with their corresponding french translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is light ?', 'Qu’est-ce que la lumière?'),\n",
       " ('Who are we?', 'Où sommes-nous?'),\n",
       " ('Where did we come from?', \"D'où venons-nous?\"),\n",
       " ('What would we do without it?', 'Que ferions-nous sans elle ?'),\n",
       " ('What is the absolute location (latitude and longitude) of Badger, Newfoundland and Labrador?',\n",
       "  'Quelle sont les coordonnées (latitude et longitude) de Badger, à Terre-Neuve-etLabrador?')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(questions, open(f'{PATH}/data/translate/en-fr-questions.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "questions = pickle.load(open(f'{PATH}/data/translate/en-fr-questions.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "en_questions, fr_questions = zip(*questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "re_apos = re.compile(r\"(\\w)'s\\b\")         # make 's a separate word\n",
    "re_mw_punc = re.compile(r\"(\\w[’'])(\\w)\")  # other ' in a word creates 2 words\n",
    "re_punc = re.compile(\"([\\\"().,;:/_?!—])\") # add spaces around punctuation\n",
    "re_mult_space = re.compile(r\"  *\")        # replace multiple spaces with just one\n",
    "\n",
    "def tokenize(sent):\n",
    "    sent = re_apos.sub(r\"\\1 's\", sent)\n",
    "    sent = re_mw_punc.sub(r\"\\1 \\2\", sent)\n",
    "    sent = re_punc.sub(r\" \\1 \", sent).replace('-', ' ')\n",
    "    sent = re_mult_space.sub(' ', sent)\n",
    "    return sent.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tokenize english questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "en_tokens = list(map(tokenize, en_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what', 'is', 'light', '?'],\n",
       " ['who', 'are', 'we', '?'],\n",
       " ['where', 'did', 'we', 'come', 'from', '?'],\n",
       " ['what', 'would', 'we', 'do', 'without', 'it', '?']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokens[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tokenize french questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fr_tokens = list(map(tokenize, fr_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['qu’', 'est', 'ce', 'que', 'la', 'lumière', '?'],\n",
       " ['où', 'sommes', 'nous', '?'],\n",
       " [\"d'\", 'où', 'venons', 'nous', '?'],\n",
       " ['que', 'ferions', 'nous', 'sans', 'elle', '?']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_tokens[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For each language: \n",
    "<br/>- Get vocabulary counter.\n",
    "<br/>- Get vocabulary.\n",
    "<br/>- Get dictionary that maps each word to an index.\n",
    "<br/>- Transform tokens to their corresponding ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PAD = 0; SOS = 1\n",
    "\n",
    "def tokens2ids(sentences):\n",
    "    vocab_counter = collections.Counter(word for sent in sentences for word in sent)\n",
    "    vocab = sorted(vocab_counter, key=vocab_counter.get, reverse=True)\n",
    "    vocab.insert(PAD, '<PAD>')\n",
    "    vocab.insert(SOS, '<SOS')\n",
    "    w2id = {word:i for i, word in enumerate(vocab)}\n",
    "    ids = [[w2id[word] for word in sent] for sent in sentences]\n",
    "    return vocab_counter, vocab, w2id, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "en_vocab_counter, en_vocab, en_w2id, en_ids = tokens2ids(en_tokens)\n",
    "fr_vocab_counter, fr_vocab, fr_w2id, fr_ids = tokens2ids(fr_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19549, 26709)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_vocab), len(fr_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will not train word vectors from scratch, we will use <a href='http://nlp.stanford.edu/data/glove.6B.zip'>GloVe</a> for english words and <a href='http://fauconnier.github.io/index.html#wordembeddingmodels'>FrWac2Vec</a> for french words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### GloVe preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will:\n",
    "<br/>- Load words.\n",
    "<br/>- Assign an index to each word.\n",
    "<br/>- Create a dictionary that maps each word to their ids.\n",
    "<br/>- Create an array with numeric vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "glove_path = f'{PATH}/data/glove'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2id = {}\n",
    "word_vectors = bcolz.carray(np.zeros(1), rootdir=f'{glove_path}/results/6B.100.dat', mode='w')\n",
    "\n",
    "with open(f'{glove_path}/glove.6B.100d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2id[word] = idx\n",
    "        idx += 1\n",
    "        word_vector = np.array(line[1:]).astype(np.float)\n",
    "        word_vectors.append(word_vector)\n",
    "    #word_vectors.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_vectors = bcolz.carray(word_vectors[1:].reshape((400000, 100)), rootdir=f'{glove_path}/results/6B.100.dat', mode='w')\n",
    "word_vectors.flush()\n",
    "pickle.dump(words, open(f'{glove_path}/results/6B.100_words.pkl', 'wb'))\n",
    "pickle.dump(word2id, open(f'{glove_path}/results/6B.100_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load english words vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# English words vectors from GloVe (numeric vectors).\n",
    "glove_vectors = bcolz.open(f'{glove_path}/results/6B.100.dat')[:]\n",
    "# English words from GloVe (list of strings).\n",
    "glove_words = pickle.load(open(f'{glove_path}/results/6B.100_words.pkl', 'rb'))\n",
    "# Dictionary that maps each english word from GloVe to their corresponding ids.\n",
    "glove_word2id = pickle.load(open(f'{glove_path}/results/6B.100_idx.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will create a dictionary that maps each glove english word to their corresponding numeric vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "glove_word2vec = {word: glove_vectors[glove_word2id[word]] for word in glove_words}\n",
    "n_glove_vectors, dim_glove_vectors = glove_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_glove_vectors, dim_glove_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have 400000 glove english words vectors with dimension equal to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.1764e-02, -6.0768e-01,  5.2233e-01, -1.1533e-02,  3.6009e-01,\n",
       "        3.6460e-01, -4.9728e-03, -3.3769e-04,  6.6011e-01, -1.2602e-01,\n",
       "        2.3832e-01,  5.6113e-02, -1.1328e-01,  3.5199e-01,  2.4070e-01,\n",
       "       -2.9588e-01, -3.1811e-01,  7.9509e-03,  3.2996e-01, -1.0383e-01,\n",
       "       -4.0230e-01, -3.7351e-03,  4.7088e-01,  2.2141e-01,  3.3043e-01,\n",
       "       -4.5048e-01,  3.5376e-01,  5.5943e-01,  2.3509e-01,  5.0190e-02,\n",
       "        5.7384e-01,  9.1137e-01,  8.9360e-01,  1.3000e-01,  6.7807e-01,\n",
       "        4.1787e-01, -6.9812e-01, -6.0581e-01,  1.1147e+00, -4.3455e-03,\n",
       "        4.6439e-01, -3.8663e-01,  9.2078e-02, -3.5278e-01, -9.2302e-01,\n",
       "        3.7423e-02, -4.9481e-01, -2.0403e-01,  8.0609e-01, -6.7063e-01,\n",
       "        1.9324e-01,  6.9329e-01,  8.1743e-01,  3.7762e-01,  2.6951e-01,\n",
       "       -1.7669e+00, -7.0825e-01,  2.7024e-01,  1.9455e+00,  7.2376e-01,\n",
       "        1.7558e-01, -1.7475e-01,  1.7004e-01, -6.7982e-01, -2.3057e-01,\n",
       "        8.5733e-02,  7.8184e-01,  3.4410e-01,  8.3690e-01,  2.7714e-01,\n",
       "       -4.1911e-01, -1.3996e-02,  3.6299e-01, -1.2430e+00,  4.0935e-01,\n",
       "        8.2805e-01,  6.8355e-01, -2.0146e-01, -9.2850e-01, -7.1892e-02,\n",
       "        1.4195e+00, -1.0319e+00, -1.1143e-01, -2.4394e-01, -2.1674e+00,\n",
       "       -2.7823e-01,  4.8543e-01, -1.1831e-01, -1.0966e-01, -2.0811e-01,\n",
       "        4.4119e-01,  9.3765e-01,  6.7985e-01, -5.2194e-01, -4.1710e-01,\n",
       "        2.7298e-01,  3.7404e-02,  1.0726e-01,  8.0893e-01,  2.4408e-02])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_word2vec['phone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For instance this is 'phone' vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### French word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fr_w2v_path = f'{PATH}/data/frWac_non_lem_no_postag_no_phrase_200_skip_cut100.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fr_w2v = KeyedVectors.load_word2vec_format(fr_w2v_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_w2v.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "French word vectors dim is equal to 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we need to create embeddings matrices for english and french words of training corpus. If a word appears on GloVe or frWac then we load its pre-trained vector, otherwise we create a random vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_embedding(w2v, target_vocab, emb_dim):\n",
    "    emb_len = len(target_vocab)\n",
    "    embedding = np.zeros((emb_len, emb_dim))\n",
    "    words_found = 0\n",
    "    \n",
    "    for i, w in enumerate(target_vocab):\n",
    "        try: \n",
    "            embedding[i] = w2v[w]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            embedding[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "    \n",
    "    return embedding, words_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "en_emb, words_found = create_embedding(glove_word2vec, en_vocab, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19549, 100), 17251)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_emb.shape, words_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fr_emb, words_found = create_embedding(fr_w2v, fr_vocab, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26709, 200), 21878)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_emb.shape, words_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Min, max and mean length of english sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 202, 13.153904951176168)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_en_ids = [len(sentence) for sentence in en_ids]\n",
    "min(len_en_ids), max(len_en_ids), np.mean(len_en_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Min, max and mean length of french sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 189, 15.776442261756893)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_fr_ids = [len(sentence) for sentence in fr_ids]\n",
    "min(len_fr_ids), max(len_fr_ids), np.mean(len_fr_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We set 30 as max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maxlen = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "en_padded = pad_sequences(en_ids, maxlen, 'int64', 'post', 'post')\n",
    "fr_padded = pad_sequences(fr_ids, maxlen, 'int64', 'post', 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52331, 30), (52331, 30), (19549, 100), (26709, 200))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_padded.shape, en_padded.shape, en_emb.shape, fr_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have: \n",
    "<br/>- 52331 english questions with their 52331 french translations (both with max length = 30).\n",
    "<br/>- 19549 english words vectors with dim 100.\n",
    "<br/>- 26709 french words vectors with dim 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We split dataset into 90% training - 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "en_train, en_test, fr_train, fr_test = train_test_split(en_padded, fr_padded, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(47097, 30), (5234, 30), (47097, 30), (5234, 30)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.shape for s in [fr_train, fr_test, en_train, en_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For instance, this is the first english question in training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  15,    8,    3,   73,    5,   41, 2699,    3,  134,  848,    6,\n",
       "       1442, 1983,   13,  182,  102,  856,    2,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each of those integers represent a word. We then look up into the embedding matrix to get their word vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach is the creation of a simple sequence to sequence model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>Encoder:\n",
    "<br/>\n",
    "<br/>Inputs: french words sentence and initial hidden state (all zeros).\n",
    "<br/>1- Look up at an embedding layer to get word vector of each word of the input sentence. \n",
    "<br/>2- Pass the word vectors sequence through a RNN.\n",
    "<br/>3- Return hidden state of last timestep (vector representation of input sentence).\n",
    "<br/>\n",
    "<br/>Decoder:\n",
    "<br/>\n",
    "<br/>Inputs: 'SOS' word (i.e. start of sentence, is always the first word) and vector representation created by encoder.\n",
    "<br/>1- Load vector representation as initial hidden state.\n",
    "<br/>2- Look up at an embedding layer to get word vector of 'SOS'.\n",
    "<br/>3- Pass the word vector through a RNN.\n",
    "<br/>4- Generate prediction of next word.\n",
    "<br/>5- Repeat 2, 3 and 4 using always the previous translated word until finish sentence translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fr_emb_t = torch.FloatTensor(fr_emb).cuda()\n",
    "en_emb_t = torch.FloatTensor(en_emb).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def long_t(arr):\n",
    "    return Variable(torch.LongTensor(arr)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained vectors into an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emb(emb_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = emb_matrix.size()\n",
    "    emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb.load_state_dict({'weight': emb_matrix})\n",
    "    if non_trainable:\n",
    "        #emb.weight.requires_grad = False\n",
    "        for param in emb.parameters():\n",
    "            param.requires_grad = False\n",
    "    return emb, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size, num_layers=2):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # Create embedding layer.\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb(emb_matrix, True)\n",
    "        # Create RNN.\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, inp, hidden):\n",
    "        return self.gru(self.embedding(inp), hidden)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size, num_layers=2):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # Create embedding layer.\n",
    "        self.emb, num_embeddings, embedding_dim = create_emb(emb_matrix)\n",
    "        # Create RNN.\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "        self.out = nn.Linear(hidden_size, num_embeddings)\n",
    "        \n",
    "    def forward(self, inp, hidden):\n",
    "        emb = self.emb(inp).unsqueeze(1)\n",
    "        res, hidden = self.gru(emb, hidden)\n",
    "        # Softmax layer, generates probs for each word vector of the embedding layer.\n",
    "        res = F.log_softmax(self.out(res[:,0]), dim=1)\n",
    "        return res, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(inp, encoder):\n",
    "    batch_size, input_length = inp.size()\n",
    "    hidden = encoder.init_hidden(batch_size).cuda()\n",
    "    enc_outputs, hidden = encoder.forward(inp, hidden)\n",
    "    return long_t([SOS]*batch_size), enc_outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use teaching-force as training approach. Rather than pass to decoder the previous translated word, we pass the real target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(encoder, decoder, train_dl, valid_dl, n_epochs, enc_optim, dec_optim, criterion):\n",
    "    bar = tqdm_notebook(total=n_epochs)\n",
    "    \n",
    "    avg_mom = 0.98\n",
    "    avg_loss = 0.\n",
    "    batch_num = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        bar2 = tqdm_notebook(total=train_dl.dataset.shape[0] / train_dl.batch_size, desc=f'Epoch {epoch}', leave=False)\n",
    "        for i, batch in enumerate(train_dl):\n",
    "            batch_num += 1\n",
    "            loss = 0\n",
    "            \n",
    "            inp = long_t(batch[:, :30])\n",
    "            targ = long_t(batch[:, 30:])\n",
    "        \n",
    "            # Encoder creates a vector representation of input french sentence. \n",
    "            decoder_input, encoder_output, hidden = encode(inp, encoder)\n",
    "\n",
    "            # Zero the gradients before running the backward pass.\n",
    "            enc_optim.zero_grad()\n",
    "            dec_optim.zero_grad()\n",
    "            \n",
    "            targ_length = targ.size()[1]\n",
    "            \n",
    "            for di in range(targ_length):\n",
    "                decoder_output, hidden = decoder(decoder_input, hidden)\n",
    "                # Teacher forcing: the decoder receives as input the real target instead of predicted word.\n",
    "                decoder_input = targ[:, di]\n",
    "                # Compute loss.\n",
    "                loss += criterion(decoder_output, decoder_input)\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to all the learnable parameters of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its parameters.\n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "            \n",
    "            # Exponentially weighted moving average, to make the reported loss more stable.\n",
    "            avg_loss = avg_loss * avg_mom + (loss.data[0] / targ_length)  * (1-avg_mom)\n",
    "            \n",
    "            # Compute bias-corrected loss estimate.\n",
    "            debias_loss = avg_loss / (1 - avg_mom**batch_num)\n",
    "            \n",
    "            bar2.update()\n",
    "            \n",
    "        # Compute validation loss.\n",
    "        #val = validate(model, valid_dl, criterion)\n",
    "        \n",
    "        print(np.round([epoch, debias_loss], 6))    \n",
    "        bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def req_grad_params(o):\n",
    "    return (param for param in o.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "encoder = EncoderRNN(fr_emb_t, hidden_size).cuda()\n",
    "decoder = DecoderRNN(en_emb_t, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_opt = optim.Adam(req_grad_params(encoder), lr=lr)\n",
    "dec_opt = optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(np.concatenate([fr_train, en_train], 1), batch_size, shuffle=True, num_workers=1)\n",
    "valid_dl = DataLoader(np.concatenate([fr_test, en_test], 1), batch_size * 2, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a046149e4e4afdb51147618006f795"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2683df33f49243b0aa17d7504d5e3a70"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.       2.029788]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e16be8bed764fbd83a33076b186da08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.       1.828452]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db1929493b54320b39eb2c344894ccf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.       1.687602]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c289038d9a0f45568368e733474436a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.      1.60677]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4955935b72422893c555d3ad1632e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.       1.502757]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e3545827804dcabba7d8ee7733bcf0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.       1.457319]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4331ab59014c28be825951b871f557"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.       1.445323]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa84f531ce64ad3a53d209da7c287f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.       1.418944]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcc22d5c57146c99b9c3a63dfb4f17e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.       1.384115]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f80505207942be8d05b27bde22cdfa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.       1.361875]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f1193d4e424b26a9e0a3739c6e23b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.        1.370231]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c0b588fd4d4b64b6fdf10628b32c26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.        1.333297]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af82b736e5ff4e899a4a633a2fa7eb82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.        1.312195]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591dbce714824a3289bb3c7651a9707b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.        1.317822]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21562571390a4b95ba7ca9c4e9c6c3aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.        1.332063]\n"
     ]
    }
   ],
   "source": [
    "fit(encoder, decoder, train_dl, valid_dl, 15, enc_opt, dec_opt, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate predictions of a french sentence:\n",
    "<br/>1- Tokenize.\n",
    "<br/>2- Transform words to their ids.\n",
    "<br/>3- Set sentence length = 30.\n",
    "<br/>3- Encode.\n",
    "<br/>4- Decode next translated word until the decoder generates a special word that means end of sentence or until reach the max length = 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2ids(sent):\n",
    "    ids = [fr_w2id[t] for t in tokenize(sent)]\n",
    "    return pad_sequences([ids], maxlen, 'int64', 'post', 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(inp):\n",
    "    decoder_input, encoder_outputs, hidden = encode(inp, encoder)\n",
    "    target_length = maxlen\n",
    "    \n",
    "    decoded_words = []\n",
    "    for di in range(target_length):\n",
    "        decoder_output, hidden = decoder(decoder_input, hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni==PAD:\n",
    "            break\n",
    "        decoded_words.append(en_vocab[ni])\n",
    "        decoder_input = long_t([ni])\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fr2en(sent):\n",
    "    ids = long_t(sent2ids(sent))\n",
    "    translation = evaluate(ids)\n",
    "    return ' '.join(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"D'où venons-nous?\", 'Where did we come from?', 'where do we come from ?')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 2\n",
    "fr_questions[i], en_questions[i], fr2en(fr_questions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Quelle est la densité de population au Canada ?',\n",
       " \"What is Canada's population density?\",\n",
       " 'what is the size of the population of canadians ?')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 21\n",
    "fr_questions[i], en_questions[i], fr2en(fr_questions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Quels en sont les avantages pour moi?',\n",
       " \"What's in it for me?\",\n",
       " 'what are the benefits of joining ?')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 32\n",
    "fr_questions[i], en_questions[i], fr2en(fr_questions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some short sentences as above examples show reasonable translations, but for long senteces the performance is much worst. This could be improved using attention and will be the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), f'{PATH}/results/simple_encoder.pth')\n",
    "torch.save(decoder.state_dict(), f'{PATH}/results/simple_decoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
